# Running A Libra Benchmark

## General Goal of Running Benchmark

Passing all unit tests and smoke tests is good sign that everything is working. But sometimes you want more than that. Running a benchmark, beyond acting as an equivalent end-to-end testing, gives you the ability to run a pre-setup scenario and to evaluate the current performance of the local Libra blockchain network. By inspecting the metrics reported by each components, this gives you the ability to pin down potential bottleneck of the system.

This documentation describes the architecture of the benchmarking library and then introduces its most straightforward application-finding maximum throughput.
Two highly related applications of Benchmarker are discussed in [Load Generation](doc/load_generator.md) and [Protecting against Denial of Service Attack](doc/dos.md).

## Architecture of Benchmarker

Given transactions and sender accounts generated by [load_generator](doc/load_generator.md), Benchmarker plays these transactions: submit transaction to Admission Control (AC) and wait for transactions to become committed.

### Submit Transactions

1. The first series of steps are to generate test accounts, mint them with faucet account, and generate large number of transactions between generated accounts. See the [Load Generation](doc/load_generator.md) section for detail.
2. Generated transactions as input to Benchmarker are dispatched evenly to multiple AC clients.
3. AC clients send async SubmitTransactionRequests in parallel to the corresponding connected AC.
4. AC servers in validators check submitted transactions.
5. AC servers respond to AC clients, specifying whether each transaction is accepted or rejected
6. Benchmarker reports submission-phase metrics to Prometheus.

One limitation in the submission phase is the transactions are assumed to be mutually independent.

### Wait Transactions and Report Throughput

1. Benchmarker takes in all the senders’ account data, and dispatches evenly to AC clients.
2. For each sender, a gRPC request for the account’s persisted sequence number is sent to validator.
3. Validator accepts and processes the request, then
4. Responds to corresponding requesting AC client.
5. By checking against the previously requested persisted sequence number, Benchmarker infers how many transactions are committed.
6. When all transactions are committed, or the last submitted transaction expires, Benchmarker reports the experiment results (e.g., request and transaction throughput, number of committed transactions) to caller.

### Error Handling in Each Steps during Playing Transaction

* Setup stage
    * fail to creating in-sync faucet account => panic
    * fail to mint generated accounts => panic
* Generating offline transactions: ignore but report failed ones
* Submitted transactions may fail due to network/gRPC issues or rejected by AC
    * Ignore and continue to submit subsequent transactions
    * Retry failed/rejected transactions in previous submission process
* Wait transactions until the last one expires
    * Don't count uncommitted transactions when calculating throughput.
    * For each sender, set its local sequence number to be the one synchronized from validator(s).


### Report Metrics in Each Stage to Prometheus

1. transaction generation: `requested_txns` (i.e. number of transactions the user want RuBen to generate
    * success: `created_txns`
    * failure: `sign_failed_txns`
2. transaction submission: `created_txns`
    * success: `submit_txns.Accepted`
    * failure: auto-formatted based on `grpcio::Error` and `proto::SubmitTransactionResponse` (AC response) status with prefix `submit_txns`
3. Final status within an epoch: `submit_txns.Accepted `
    * success: `committed_txns`
    * failure: `timedout_txns`
4. Result gauges: `request_duration_ms`, `wait_duration_ms`, `running_duration_ms`, `request_throughput`, `txns_throughput`



## Linear Search Libra’s Maximum Throughput

Another important application of Benchmarker is to find the our system's maximum throughput. With aforementioned submission rate feature, a straightforward way to search the maximum throughput is to keep submitting transactions at a constant rate for a while and see if Libra can absorb these transactions, to commit submitted transactions before they expires.
If termination condition is not met, search continues as we increase the rate as well as the volume by a certain amount.
Here we define two termination conditions
1. if the number of uncommitted transactions is more than 30% of the submitted transactions, we believe Libra is already unable to process the injected transactions.
2. the secondary condition, a hard limit of submission rate, just gives user a little bit more control of the search.

```
// bm: Benchmarker
// generator: impl LoadGenerator
let rate = lower_bound;
while rate < upper_bound {
    // Generate and send transactions @ constant rate
    let (num_submitted, num_committed, throughput) =
        run_benchmarker_at_const_rate(bm, generator, accounts, rate);

    // Measure throughput & commit ratio over several epochs
    max_throughput = max(max_throughput, throughput);
    if (num_committed / num_submitted) < COMMIT_RATIO_THRESHOLD {
        break;
    }
    rate += inc_step
}
```

The result we used for testing Libra at a fixed submission rate is the aggregated results over several epochs,
and the maximum throughput will be reported when search terminates.

If we dive in a little bit to the testing procedure, within each epoch, we will first generate new transactions from account chunks so that any account doesn't create more than the `capacity_per_user` limit. After measuring the throughput for several epochs, we summarize the results as total number of submitted transactions, committed transactions and overall throughput, that is the total number of committed transactions divided by total submitting and waiting durations.

```
fn run_benchmarker_at_const_rate() {
    for _ in 0..num_epochs {
        // Generate new accounts
        let new_accounts = generator.gen_accounts();
        accounts.extend(new_accounts);
        // Generate new transactions from chunked accounts
        for account_chunk in accounts.chunks(batch_size) {
            let req_chunk = generator.generate_request(account_chunk);
            requests.extend(req_chunk.into_iter());
        }
        result.push(bm.measure_throughput(requests, accounts, rate));
    }
    // Return #total submitted transactions, #total committed transactions, overall throughput.
    aggregated_result(result)
}
```

A side benefit of linear search is that we can observe how the entire system react to constantly injected workload. But you can also argue it may impose too much load during the search. In addition, searching usually takes very long time.

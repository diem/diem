# Profiling MoveVM with Instrument
This doc is going to talk about how to run performance benchmarks on macOS.

## Step 1: Get latest Xcode and open Instrument

Instrument can be found in Xcode > Open Developer Tool > Instrument.
![](https://i.imgur.com/QCwJBim.png)



## Step 2: Choose a benchmark suite.

We currently have four local benchmark candidates:
- `executor_benchmark` in `diem/executor`
- `txn_bench` in `diem/language/benchmark`
- `Arith` and `call` benchmark in `diem/language/benchmark`
- `diem-swarm` with transaction traffic generated by `cluster-test`.

The first item is a comprehensive benchmark of diem adapter, executor and storage that generates a block of p2p transactions and tries to execute and commit it to the DiemDB in local storage. The second item is a benchmark of Diem adapter only with a fake executor and an in-memory storage that executes randomly generated p2p transactions. The third item, although it’s still invoking Diem adapter, is mostly testing on the MoveVM’s ability of handling simple arithmetic operations and call stacks.

`diem-swarm` is the most comprehensive suite which spawns a single validator node on your local machine. To start this suite, you will need to execute `scripts/cli/start_cli_swarm.sh` and start transaction generation by following the `cluster-test` instruction emitted in the instruction.

## Step 3: Select the running process in Instrument.
Open instrument and create a time profiler project.

![](https://i.imgur.com/dbLht9f.png)

Launch the benchmark target in the terminal, and select it in Instrument.

![](https://i.imgur.com/LU10tZC.jpg)


## Step 4: Get analysis!

Here’s an example trace from running the benchmark

![](https://i.imgur.com/BAoprNq.jpg)

# Getting Reliable Benchmark Numbers

For the benchmark suite we've implemented, we use cpu time for measuring the perf of DiemVM to help the measurement stability in a CI setup. This however introduces lots of noise when running the benchmark suite locally. Thus if you are running the benchmark on your own machine, you should turn off all other resource consuming applications like browsers, IDE, etc, and run a few times to make sure `criterion` reports " No change in performance detected". Usually the numbers should be pretty stable if the machine is mostly on idle. Here's an example output from the `criterion`:

```
peer_to_peer            time:   [281.61 ms 283.87 ms 286.56 ms]                         
                        change: [-0.6289% +0.5859% +1.8803%] (p = 0.36 > 0.05)
                        No change in performance detected.
Found 8 outliers among 100 measurements (8.00%)
  1 (1.00%) high mild
  7 (7.00%) high severe
```